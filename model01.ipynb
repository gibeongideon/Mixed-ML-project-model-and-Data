{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "from keras.models import load_model\n",
    " \n",
    "# return training data\n",
    "def get_train():\n",
    "    seq = [[0.0, 0.1], [0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5]]\n",
    "    seq = array(seq)\n",
    "    X, y = seq[:, 0], seq[:, 1]\n",
    "    X = X.reshape((len(X), 1, 1))\n",
    "    return X, y\n",
    "\n",
    "#print(X)\n",
    "#print(y)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(1,1)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# compile model\n",
    "model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "# fit model\n",
    "X,y = get_train()\n",
    "model.fit(X, y, epochs=300, shuffle=False, verbose=0)\n",
    "# save model to single file\n",
    "score = model.evaluate(X, y, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Training done')\n",
    "\n",
    "model.save('lstm_model.h5')\n",
    "print('Model Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 1\n",
    "BATCH_SIZE = 500\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "model.save('N_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 1\n",
    "BATCH_SIZE = 500\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices..\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "#___________________________________________________________________________________\n",
    "\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Loss Train\n",
    "model.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,batch_size=BATCH_SIZE, epochs=NB_EPOCH,verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "# Test\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save('g_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOMECODE</th>\n",
       "      <th>AWAYCODE</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>X</th>\n",
       "      <th>HT1</th>\n",
       "      <th>HT2</th>\n",
       "      <th>HTX</th>\n",
       "      <th>FS (0:0)</th>\n",
       "      <th>FS(0:0)2</th>\n",
       "      <th>...</th>\n",
       "      <th>G(0.5)U</th>\n",
       "      <th>G(1.5)O</th>\n",
       "      <th>G(1.5)U</th>\n",
       "      <th>G(2.5)O</th>\n",
       "      <th>G(2.5)U</th>\n",
       "      <th>G(3.5)O</th>\n",
       "      <th>G(3.5)U</th>\n",
       "      <th>G(4.5)O</th>\n",
       "      <th>G(4.5)U</th>\n",
       "      <th>FT_Results_1_0_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.20</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.05</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.40</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1.65</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.80</td>\n",
       "      <td>2.10</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.45</td>\n",
       "      <td>4.15</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.05</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1.50</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.80</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1.50</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.80</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.70</td>\n",
       "      <td>3.90</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>1.65</td>\n",
       "      <td>5.50</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1.90</td>\n",
       "      <td>5.75</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.45</td>\n",
       "      <td>3.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.35</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.20</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.05</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.45</td>\n",
       "      <td>6.25</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>6.25</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>8.25</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.15</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.35</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.85</td>\n",
       "      <td>4.05</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1.40</td>\n",
       "      <td>7.50</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.75</td>\n",
       "      <td>7.50</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.70</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.70</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>4.10</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>1.30</td>\n",
       "      <td>11.50</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.60</td>\n",
       "      <td>9.00</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1.40</td>\n",
       "      <td>7.50</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.75</td>\n",
       "      <td>7.50</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.85</td>\n",
       "      <td>4.20</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.45</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.15</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.15</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>1.30</td>\n",
       "      <td>11.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.60</td>\n",
       "      <td>8.75</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.30</td>\n",
       "      <td>4.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3.05</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.55</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.05</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>1.35</td>\n",
       "      <td>8.25</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>7.50</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1.60</td>\n",
       "      <td>5.50</td>\n",
       "      <td>4.20</td>\n",
       "      <td>1.80</td>\n",
       "      <td>6.25</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7979</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1.30</td>\n",
       "      <td>8.25</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9.50</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>4.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7986</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1.40</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.85</td>\n",
       "      <td>6.75</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>4.75</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8002</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>1.65</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8007</th>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.15</td>\n",
       "      <td>4.45</td>\n",
       "      <td>3.55</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.05</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8010</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.90</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8036</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2.55</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>4.05</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8039</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1.35</td>\n",
       "      <td>8.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.70</td>\n",
       "      <td>8.25</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.80</td>\n",
       "      <td>3.95</td>\n",
       "      <td>4.20</td>\n",
       "      <td>2.10</td>\n",
       "      <td>4.75</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8052</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8066</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.40</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8102</th>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.15</td>\n",
       "      <td>4.30</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8118</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.55</td>\n",
       "      <td>3.55</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8156</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>1.30</td>\n",
       "      <td>10.50</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.60</td>\n",
       "      <td>9.50</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.30</td>\n",
       "      <td>4.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8161</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8163</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4.05</td>\n",
       "      <td>1.85</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8166</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>4.85</td>\n",
       "      <td>5.75</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8186</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.65</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>4.40</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8212</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1.40</td>\n",
       "      <td>6.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.80</td>\n",
       "      <td>7.25</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8218</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1.40</td>\n",
       "      <td>7.25</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1.85</td>\n",
       "      <td>6.75</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8219</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.40</td>\n",
       "      <td>7.50</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1.90</td>\n",
       "      <td>6.25</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.65</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8225</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.35</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8237</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.80</td>\n",
       "      <td>3.95</td>\n",
       "      <td>4.20</td>\n",
       "      <td>2.15</td>\n",
       "      <td>4.65</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8247</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.15</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8252</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1.65</td>\n",
       "      <td>4.10</td>\n",
       "      <td>5.25</td>\n",
       "      <td>2.10</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.05</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8258</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8315</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>1.60</td>\n",
       "      <td>5.25</td>\n",
       "      <td>4.15</td>\n",
       "      <td>1.85</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.45</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8316</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1.35</td>\n",
       "      <td>8.00</td>\n",
       "      <td>5.75</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9.25</td>\n",
       "      <td>3.05</td>\n",
       "      <td>1.25</td>\n",
       "      <td>4.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>559 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HOMECODE  AWAYCODE     1      2     X   HT1   HT2   HTX  FS (0:0)  \\\n",
       "2            8         2  3.55   1.90  4.20  3.90  2.20  2.70      2.55   \n",
       "16          14         8  3.00   2.25  3.65  3.20  2.75  2.50      2.10   \n",
       "19          10        11  2.00   3.60  3.75  2.40  3.60  2.55      1.70   \n",
       "35           2         6  1.65   4.20  4.80  2.10  4.60  2.60      1.50   \n",
       "48           8         3  3.85   2.00  3.45  4.15  2.20  2.60      2.50   \n",
       "67          16         4  6.25   1.50  4.30  4.80  2.10  2.55      2.80   \n",
       "69          16         4  6.25   1.50  4.30  4.80  2.10  2.55      2.80   \n",
       "78          15         6  4.90   1.70  3.90  4.35  2.15  2.60      2.65   \n",
       "100          7        11  1.65   5.50  3.95  1.90  5.75  2.65      1.45   \n",
       "107         10         4  3.60   2.00  3.75  3.35  2.25  2.95      2.25   \n",
       "112          8         2  3.55   1.90  4.20  3.90  2.20  2.75      2.55   \n",
       "115          2        10  1.45   6.25  5.00  1.85  6.25  2.60      1.40   \n",
       "140          6        16  1.35   9.00  5.00  1.75  8.25  2.60      1.30   \n",
       "171          3         4  2.00   3.70  3.50  2.35  3.45  2.75      1.70   \n",
       "193         14         6  3.75   1.85  4.05  3.70  2.25  2.70      2.50   \n",
       "199          6        15  1.40   7.50  5.25  1.75  7.50  2.65      1.35   \n",
       "249          5         3  2.40   2.70  3.85  2.75  2.95  2.65      1.95   \n",
       "253         10         5  3.30   2.05  3.90  3.40  2.45  2.60      2.15   \n",
       "260         15         2  5.25   1.70  3.80  4.70  2.05  2.65      2.85   \n",
       "269         14         5  3.30   2.05  3.90  3.60  2.65  2.35      2.20   \n",
       "271          3        16  1.30  11.50  5.25  1.60  9.00  2.90      1.25   \n",
       "296          6        15  1.40   7.50  5.25  1.75  7.50  2.60      1.35   \n",
       "305         14         6  3.70   1.85  4.20  3.70  2.30  2.65      2.50   \n",
       "307         10        11  2.00   3.50  3.85  2.45  3.45  2.65      1.70   \n",
       "335          8         2  3.50   1.90  4.15  3.85  2.30  2.65      2.50   \n",
       "355          3        16  1.30  11.00  5.00  1.60  8.75  2.90      1.30   \n",
       "358          8         6  3.05   2.25  3.70  3.55  2.85  2.25      2.10   \n",
       "360         10         5  3.20   2.05  4.00  3.60  2.50  2.50      2.20   \n",
       "368          6        16  1.35   8.25  5.00  1.75  7.50  2.60      1.30   \n",
       "372          4        10  1.60   5.50  4.20  1.80  6.25  2.80      1.40   \n",
       "...        ...       ...   ...    ...   ...   ...   ...   ...       ...   \n",
       "7979         1        12  1.30   8.25  6.00  1.55  9.50  3.00      1.25   \n",
       "7986         6        15  1.40   7.00  5.25  1.85  6.75  2.60      1.35   \n",
       "7992        15         3  5.50   1.50  4.75  5.50  1.95  2.60      2.95   \n",
       "8002         8        16  1.65   5.00  4.00  2.00  5.00  2.60      1.40   \n",
       "8007        16        11  2.80   2.15  4.45  3.55  2.50  2.50      2.15   \n",
       "8010        15         2  5.50   1.65  3.90  5.25  1.90  2.75      3.20   \n",
       "8036         4         2  2.15   3.00  3.95  2.55  3.50  2.50      1.65   \n",
       "8039         7        15  1.35   8.00  5.00  1.70  8.25  2.65      1.35   \n",
       "8049         1         7  1.80   3.95  4.20  2.10  4.75  2.55      1.55   \n",
       "8052        10         5  3.00   2.20  3.85  3.30  2.55  2.65      2.05   \n",
       "8066        10         1  3.90   1.90  3.80  4.40  2.10  2.65      2.55   \n",
       "8102        16        11  2.85   2.15  4.30  3.70  2.50  2.45      2.20   \n",
       "8118         7         1  2.60   2.50  3.80  2.95  2.85  2.55      1.90   \n",
       "8120         4         2  2.20   3.10  3.75  2.55  3.55  2.45      1.65   \n",
       "8156         3        16  1.30  10.50  5.25  1.60  9.50  2.90      1.30   \n",
       "8161         5         6  2.00   3.50  3.80  2.50  3.65  2.45      1.70   \n",
       "8163        10         1  4.05   1.85  3.85  4.60  2.10  2.60      2.60   \n",
       "8166        15         3  5.50   1.50  4.85  5.75  1.90  2.65      2.95   \n",
       "8186         3         4  2.05   3.45  3.65  2.50  3.30  2.65      1.75   \n",
       "8212         1         9  1.40   6.50  5.50  1.80  7.25  2.55      1.30   \n",
       "8218         6        15  1.40   7.25  4.75  1.85  6.75  2.50      1.35   \n",
       "8219         2        10  1.40   7.50  4.70  1.90  6.25  2.55      1.40   \n",
       "8222         5         3  2.30   2.65  4.30  2.65  2.90  2.80      1.85   \n",
       "8225        10         4  3.60   2.10  3.45  3.35  2.45  2.70      2.15   \n",
       "8237         1         7  1.80   3.95  4.20  2.15  4.65  2.50      1.60   \n",
       "8247         4         6  2.25   3.45  3.15  2.50  3.70  2.45      1.85   \n",
       "8252         2         6  1.65   4.10  5.25  2.10  5.00  2.50      1.50   \n",
       "8258        10         5  3.10   2.10  3.95  3.40  2.40  2.70      2.20   \n",
       "8315         7        11  1.60   5.25  4.15  1.85  6.00  2.65      1.45   \n",
       "8316         1        12  1.35   8.00  5.75  1.55  9.25  3.05      1.25   \n",
       "\n",
       "      FS(0:0)2        ...         G(0.5)U  G(1.5)O  G(1.5)U  G(2.5)O  G(2.5)U  \\\n",
       "2         1.55        ...             0.0     1.25     3.60     1.75     2.00   \n",
       "16        1.85        ...             0.0     1.25     3.60     1.75     2.00   \n",
       "19        2.35        ...             0.0     1.25     3.60     1.75     1.95   \n",
       "35        2.65        ...             0.0     1.20     4.00     1.75     2.00   \n",
       "48        1.65        ...             0.0     1.25     3.50     1.80     1.90   \n",
       "67        1.45        ...             0.0     1.22     3.90     1.75     2.00   \n",
       "69        1.45        ...             0.0     1.22     3.90     1.75     2.00   \n",
       "78        1.55        ...             0.0     1.20     4.00     1.80     1.90   \n",
       "100       3.10        ...             0.0     1.25     3.60     1.80     1.90   \n",
       "107       1.70        ...             0.0     1.22     3.95     1.75     1.95   \n",
       "112       1.60        ...             0.0     1.25     3.65     1.75     2.00   \n",
       "115       3.40        ...             0.0     1.30     3.30     1.80     1.95   \n",
       "140       3.85        ...             0.0     1.25     3.60     1.80     1.95   \n",
       "171       2.20        ...             0.0     1.18     4.25     1.75     2.00   \n",
       "193       1.60        ...             0.0     1.22     3.85     1.75     1.95   \n",
       "199       3.40        ...             0.0     1.22     3.75     1.75     1.95   \n",
       "249       2.00        ...             0.0     1.25     3.55     1.75     1.95   \n",
       "253       1.75        ...             0.0     1.22     3.70     1.80     1.90   \n",
       "260       1.55        ...             0.0     1.20     4.10     1.75     1.95   \n",
       "269       1.80        ...             0.0     1.25     3.60     1.80     1.90   \n",
       "271       4.25        ...             0.0     1.22     3.75     1.80     1.95   \n",
       "296       3.50        ...             0.0     1.22     3.85     1.75     1.95   \n",
       "305       1.60        ...             0.0     1.22     3.85     1.75     1.95   \n",
       "307       2.30        ...             0.0     1.25     3.55     1.75     2.00   \n",
       "335       1.60        ...             0.0     1.25     3.60     1.75     1.95   \n",
       "355       4.15        ...             0.0     1.22     3.80     1.80     1.90   \n",
       "358       1.85        ...             0.0     1.25     3.50     1.75     2.00   \n",
       "360       1.75        ...             0.0     1.22     3.75     1.80     1.90   \n",
       "368       3.80        ...             0.0     1.25     3.60     1.80     1.95   \n",
       "372       3.30        ...             0.0     1.22     3.75     1.80     1.90   \n",
       "...        ...        ...             ...      ...      ...      ...      ...   \n",
       "7979      4.45        ...             0.0     1.22     3.75     1.75     1.95   \n",
       "7986      3.40        ...             0.0     1.25     3.40     1.75     1.95   \n",
       "7992      1.45        ...             0.0     1.25     3.40     1.75     1.95   \n",
       "8002      3.10        ...             0.0     1.20     3.95     1.80     1.90   \n",
       "8007      1.70        ...             0.0     1.25     3.55     1.80     1.90   \n",
       "8010      1.45        ...             0.0     1.25     3.70     1.75     2.00   \n",
       "8036      2.25        ...             0.0     1.20     4.05     1.75     1.95   \n",
       "8039      3.70        ...             0.0     1.22     3.75     1.80     1.90   \n",
       "8049      2.65        ...             0.0     1.22     3.70     1.75     1.95   \n",
       "8052      1.90        ...             0.0     1.22     3.70     1.80     1.90   \n",
       "8066      1.55        ...             0.0     1.22     3.85     1.75     2.00   \n",
       "8102      1.70        ...             0.0     1.25     3.55     1.75     1.95   \n",
       "8118      2.00        ...             0.0     1.22     3.75     1.80     1.90   \n",
       "8120      2.25        ...             0.0     1.22     3.95     1.80     1.90   \n",
       "8156      4.15        ...             0.0     1.25     3.65     1.80     1.90   \n",
       "8161      2.25        ...             0.0     1.22     3.90     1.75     1.95   \n",
       "8163      1.55        ...             0.0     1.22     3.85     1.75     2.00   \n",
       "8166      1.45        ...             0.0     1.25     3.45     1.75     1.95   \n",
       "8186      2.15        ...             0.0     1.18     4.40     1.75     1.95   \n",
       "8212      3.80        ...             0.0     1.22     3.80     1.75     2.00   \n",
       "8218      3.40        ...             0.0     1.30     3.35     1.80     1.90   \n",
       "8219      3.40        ...             0.0     1.25     3.40     1.80     1.90   \n",
       "8222      2.05        ...             0.0     1.25     3.60     1.80     1.95   \n",
       "8225      1.85        ...             0.0     1.20     3.95     1.80     1.90   \n",
       "8237      2.60        ...             0.0     1.22     3.85     1.75     2.00   \n",
       "8247      2.10        ...             0.0     1.25     3.60     1.80     1.90   \n",
       "8252      2.65        ...             0.0     1.22     3.85     1.80     1.90   \n",
       "8258      1.75        ...             0.0     1.22     3.75     1.80     1.95   \n",
       "8315      3.00        ...             0.0     1.25     3.70     1.80     1.95   \n",
       "8316      4.50        ...             0.0     1.25     3.70     1.80     1.95   \n",
       "\n",
       "      G(3.5)O  G(3.5)U  G(4.5)O  G(4.5)U  FT_Results_1_0_2  \n",
       "2        3.05     1.35      0.0      0.0                 0  \n",
       "16       2.75     1.40      0.0      0.0                 2  \n",
       "19       2.85     1.40      0.0      0.0                 1  \n",
       "35       2.85     1.35      0.0      0.0                 1  \n",
       "48       3.05     1.35      0.0      0.0                 0  \n",
       "67       2.75     1.40      0.0      0.0                 2  \n",
       "69       2.75     1.40      0.0      0.0                 2  \n",
       "78       2.95     1.35      0.0      0.0                 2  \n",
       "100      3.00     1.35      0.0      0.0                 1  \n",
       "107      2.90     1.35      0.0      0.0                 2  \n",
       "112      3.05     1.35      0.0      0.0                 2  \n",
       "115      3.20     1.30      0.0      0.0                 1  \n",
       "140      3.15     1.30      0.0      0.0                 2  \n",
       "171      2.80     1.40      0.0      0.0                 1  \n",
       "193      2.85     1.35      0.0      0.0                 0  \n",
       "199      2.65     1.45      0.0      0.0                 1  \n",
       "249      2.90     1.35      0.0      0.0                 1  \n",
       "253      3.20     1.30      0.0      0.0                 1  \n",
       "260      2.85     1.35      0.0      0.0                 2  \n",
       "269      3.10     1.30      0.0      0.0                 0  \n",
       "271      3.10     1.30      0.0      0.0                 1  \n",
       "296      2.70     1.40      0.0      0.0                 2  \n",
       "305      2.95     1.35      0.0      0.0                 2  \n",
       "307      2.80     1.40      0.0      0.0                 0  \n",
       "335      3.15     1.30      0.0      0.0                 0  \n",
       "355      3.20     1.30      0.0      0.0                 0  \n",
       "358      2.85     1.40      0.0      0.0                 1  \n",
       "360      3.20     1.30      0.0      0.0                 2  \n",
       "368      3.25     1.30      0.0      0.0                 1  \n",
       "372      3.00     1.35      0.0      0.0                 0  \n",
       "...       ...      ...      ...      ...               ...  \n",
       "7979     3.00     1.35      0.0      0.0                 1  \n",
       "7986     2.70     1.40      0.0      0.0                 1  \n",
       "7992     3.00     1.35      0.0      0.0                 1  \n",
       "8002     3.00     1.35      0.0      0.0                 1  \n",
       "8007     3.05     1.35      0.0      0.0                 2  \n",
       "8010     2.90     1.35      0.0      0.0                 1  \n",
       "8036     3.20     1.30      0.0      0.0                 0  \n",
       "8039     3.25     1.30      0.0      0.0                 1  \n",
       "8049     2.90     1.35      0.0      0.0                 0  \n",
       "8052     3.30     1.30      0.0      0.0                 2  \n",
       "8066     2.85     1.35      0.0      0.0                 2  \n",
       "8102     3.00     1.35      0.0      0.0                 2  \n",
       "8118     3.00     1.35      0.0      0.0                 1  \n",
       "8120     3.25     1.30      0.0      0.0                 2  \n",
       "8156     3.10     1.30      0.0      0.0                 1  \n",
       "8161     2.75     1.40      0.0      0.0                 0  \n",
       "8163     2.85     1.40      0.0      0.0                 2  \n",
       "8166     3.00     1.35      0.0      0.0                 2  \n",
       "8186     2.85     1.35      0.0      0.0                 2  \n",
       "8212     3.15     1.30      0.0      0.0                 2  \n",
       "8218     2.85     1.35      0.0      0.0                 2  \n",
       "8219     3.10     1.30      0.0      0.0                 1  \n",
       "8222     2.85     1.35      0.0      0.0                 1  \n",
       "8225     3.00     1.35      0.0      0.0                 2  \n",
       "8237     2.95     1.35      0.0      0.0                 2  \n",
       "8247     2.90     1.35      0.0      0.0                 1  \n",
       "8252     3.05     1.35      0.0      0.0                 0  \n",
       "8258     3.25     1.30      0.0      0.0                 2  \n",
       "8315     2.85     1.35      0.0      0.0                 1  \n",
       "8316     2.95     1.35      0.0      0.0                 1  \n",
       "\n",
       "[559 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd   \n",
    "\n",
    "df1= pd.read_csv('/home/gsociety/Desktop/The Model/STORAGE/ProcessedDATAZ.csv')\n",
    "df1= df1[df1['G(2.5)O'] >1.7]\n",
    "\n",
    "df1= df1[df1['G(2.5)O'] < 1.85]\n",
    "df1\n",
    "f=df1.drop_duplicates()\n",
    "f.head()\n",
    "\n",
    "model_cols=['HOMECODE', 'AWAYCODE', '1',\n",
    "       '2', 'X', 'HT1', 'HT2', 'HTX', 'FS (0:0)', 'FS(0:0)2', 'FS(0:0)X',\n",
    "       'G(0.5)O', 'G(0.5)U', 'G(1.5)O', 'G(1.5)U', 'G(2.5)O', 'G(2.5)U',\n",
    "       'G(3.5)O', 'G(3.5)U', 'G(4.5)O', 'G(4.5)U','FT_Results_1_0_2']\n",
    "mod_data=f.reindex(columns=model_cols).fillna(0)\n",
    "####################################\n",
    "t=f.index[-1]+1\n",
    "dataR=mod_data.values\n",
    "dataR.shape\n",
    "mod_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 503 Test Data: 56\n"
     ]
    }
   ],
   "source": [
    "t=559\n",
    "def vfl_data(t,p):\n",
    "    n=int(((100-p)/100)*t)\n",
    "    print('Train Data:',t-(t-n),'Test Data:',t-n)\n",
    "    dataTr=dataR[:n]\n",
    "    dataTe=dataR[n:]\n",
    "    \n",
    "    x_train=dataTr[:,0:21]\n",
    "    x2_train=x_train.reshape(n,7,3)\n",
    "    #x_train = x2_train[:, np.newaxis, :,:]\n",
    "    y_train= dataTr[:,21]\n",
    "\n",
    "\n",
    "    x1_test=dataTe[:,0:21]\n",
    "    #x2_test=x1_test.reshape(t-n,7,3)\n",
    "   # x_test = x2_test[:, np.newaxis, :,:]\n",
    "    x_test = x1_test\n",
    "    y_test= dataTe[:,21]\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "(x_train, y_train), (x_test, y_test)=vfl_data(t,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.  ,   7.  ,   1.75, ...,   1.35,   0.  ,   0.  ],\n",
       "       [  4.  ,   2.  ,   2.2 , ...,   1.3 ,   0.  ,   0.  ],\n",
       "       [  1.  ,  12.  ,   1.35, ...,   1.35,   0.  ,   0.  ],\n",
       "       ..., \n",
       "       [ 10.  ,   5.  ,   3.1 , ...,   1.3 ,   0.  ,   0.  ],\n",
       "       [  7.  ,  11.  ,   1.6 , ...,   1.35,   0.  ,   0.  ],\n",
       "       [  1.  ,  12.  ,   1.35, ...,   1.35,   0.  ,   0.  ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 6526 Test Data: 726\n"
     ]
    }
   ],
   "source": [
    "def vfl_data(t,p):\n",
    "    n=int(((100-p)/100)*t)\n",
    "    print('Train Data:',t-(t-n),'Test Data:',t-n)\n",
    "    dataTr=dataR[:n]\n",
    "    dataTe=dataR[n:]\n",
    "    \n",
    "    x_train=dataTr[:,0:21]\n",
    "    x2_train=x_train.reshape(n,7,3)\n",
    "    x_train = x2_train[:, np.newaxis, :,:]\n",
    "    y_train= dataTr[:,21]\n",
    "\n",
    "\n",
    "    x1_test=dataTe[:,0:21]\n",
    "    x2_test=x1_test.reshape(t-n,7,3)\n",
    "    x_test = x2_test[:, np.newaxis, :,:]\n",
    "    y_test= dataTe[:,21]\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "(x_train, y_train), (x_test, y_test)=vfl_data(t,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 6526 Test Data: 726\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "\n",
    "\n",
    "#np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 1\n",
    "BATCH_SIZE = 100\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 3 # number of outputs = number of digits\n",
    "OPTIMIZER =Adam() #SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 100\n",
    "N_HIDDEN1 =200\n",
    "#INITIAL_EPOCH = 0\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.05\n",
    "# data: shuffled and split between train and test sets\n",
    "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test)=vfl_data(t,10)\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 21\n",
    "#\n",
    "X_train = x_train.reshape(x_train.shape[0], RESHAPED)\n",
    "X_test = x_test.reshape(x_test.shape[0], RESHAPED)\n",
    "X_train = x_train.astype('float32')\n",
    "X_test = x_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 25\n",
    "X_test /= 25\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices..\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "#___________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 100)               2200      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 603       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 23,003\n",
      "Trainable params: 23,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5220 samples, validate on 1306 samples\n",
      "Epoch 1/1\n",
      "5220/5220 [==============================] - 2s 331us/step - loss: 1.0274 - acc: 0.4875 - val_loss: 0.9805 - val_acc: 0.5467\n",
      "726/726 [==============================] - 0s 72us/step\n",
      "Test score: 0.995608705791\n",
      "Test accuracy: 0.512396694872\n"
     ]
    }
   ],
   "source": [
    "# Loss Train\n",
    "model.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,batch_size=BATCH_SIZE, epochs=NB_EPOCH,verbose=VERBOSE,\n",
    "                    validation_split=VALIDATION_SPLIT)\n",
    "# Test\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#model.save('grt1_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('grt3_model.h5') Test score: 0.985016593891\n",
    "#Test accuracy: 0.551622419934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('grt2_model.h5')   # Adam Opt with Test score: 0.980862800702 Test accuracy: 0.545722714919"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "#model = load_model('g1_model.h5')\n",
    "\n",
    "model = load_model('grt3_model.h5') ##Keep this MODEL_Magic begins begin=55 end=45\n",
    "begin=50\n",
    "end=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(x_train, y_train), (x_test, y_test)=vfl_data(t,10)\n",
    "\n",
    "# consider them as float and normalize\n",
    "X_train= (x_train.astype('float32'))/25\n",
    "X_test= (x_test.astype('float32'))/25\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainp = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_testp = np_utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=X_test[-begin:-end]\n",
    "\n",
    "\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_propability = model.predict(p)\n",
    "p_class=model.predict_classes(p)\n",
    "dT=f[-begin:-end][['TC','1','2','X','HT1', 'HT2','HTX','FTR']].values\n",
    "dT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print('    Properbility to win  predictions  ')\n",
    "print('    DRAW X       HOME 1      AWAY 2    Actual results   Prediction ' )\n",
    "pr=p_propability\n",
    "\n",
    "Aarr=y_test[-begin:-end].astype('int').reshape(10,1)\n",
    "Parr=p_class.reshape(10,1)\n",
    "rsp=np.hstack((Aarr, Parr))\n",
    "\n",
    "print(np.column_stack((pr, rsp)),'\\n')\n",
    "\n",
    "print('                     1   2    X  HT1  HT2  HTX')\n",
    "print(dT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  DRAW X       HOME 1      AWAY 2    Actual results   Prediction \n",
    "[[ 0.10690524  0.67320687  0.21988785  1.          1.        ]\n",
    " [ 0.27846366  0.36485952  0.35667682  0.          1.        ]\n",
    " [ 0.22405934  0.59057975  0.18536094  1.          1.        ]\n",
    " [ 0.17733696  0.60752302  0.21514003  0.          1.        ]\n",
    " [ 0.22749841  0.23089553  0.54160607  2.          2.        ]\n",
    " [ 0.26985383  0.24772459  0.48242158  0.          2.        ]\n",
    " [ 0.16663697  0.71017379  0.1231893   1.          1.        ]\n",
    " [ 0.14986202  0.72623557  0.12390244  1.          1.        ]\n",
    " [ 0.20847422  0.17074652  0.62077928  2.          2.        ]\n",
    " [ 0.25904372  0.16915928  0.57179695  2.          2.        ]] \n",
    "\n",
    "\n",
    "    DRAW X       HOME 1      AWAY 2    Actual results   Prediction \n",
    "[[ 0.18680245  0.64725691  0.16594061  1.          1.        ]\n",
    " [ 0.28750446  0.36697137  0.34552419  0.          1.        ]\n",
    " [ 0.21265291  0.61283118  0.17451584  1.          1.        ]\n",
    " [ 0.23095743  0.55967277  0.20936985  0.          1.        ]\n",
    " [ 0.27350393  0.30446854  0.42202756  2.          2.        ]\n",
    " [ 0.27768666  0.30148607  0.4208273   0.          2.        ]\n",
    " [ 0.17786801  0.68591368  0.13621823  1.          1.        ]\n",
    " [ 0.20378891  0.62229699  0.17391412  1.          1.        ]\n",
    " [ 0.26258495  0.27056918  0.46684587  2.          2.        ]\n",
    " [ 0.26391774  0.2686632   0.46741906  2.          2.        ]] \n",
    "\n",
    "\n",
    "    DRAW X       HOME 1      AWAY 2    Actual results   Prediction \n",
    "[[ 0.11370065  0.80716121  0.07913812  1.          1.        ]\n",
    " [ 0.25868469  0.44218031  0.299135    0.          1.        ]\n",
    " [ 0.17687486  0.70732999  0.11579518  1.          1.        ]\n",
    " [ 0.17527853  0.68934625  0.1353752   0.          1.        ]\n",
    " [ 0.25514284  0.35672623  0.3881309   2.          2.        ]\n",
    " [ 0.2668373   0.33592269  0.39724001  0.          2.        ]\n",
    " [ 0.13751657  0.78461415  0.07786927  1.          1.        ]\n",
    " [ 0.14880753  0.75478828  0.09640429  1.          1.        ]\n",
    " [ 0.2538482   0.32150847  0.42464337  2.          2.        ]\n",
    " [ 0.26260597  0.31614071  0.42125338  2.          2.        ]]\n",
    "\n",
    "   DRAW X       HOME 1      AWAY 2    Actual results   Prediction \n",
    "[[ 0.24236812  0.53879124  0.21884067  1.          1.        ]\n",
    " [ 0.28034353  0.3445785   0.37507802  0.          2.        ]\n",
    " [ 0.25730363  0.50385612  0.23884025  1.          1.        ]\n",
    " [ 0.2690863   0.44864562  0.28226808  0.          1.        ]\n",
    " [ 0.27136639  0.29386136  0.43477228  2.          2.        ]\n",
    " [ 0.27638438  0.31150529  0.41211024  0.          2.        ]\n",
    " [ 0.22729056  0.5937041   0.17900535  1.          1.        ]\n",
    " [ 0.24565449  0.53659159  0.21775383  1.          1.        ]\n",
    " [ 0.26395112  0.26917207  0.46687675  2.          2.        ]\n",
    " [ 0.26640347  0.27290723  0.46068937  2.          2.        ]] \n",
    "                     1   2    X  HT1  HT2  HTX\n",
    "[['MADRID STOCKHOLM' 1.35 6.75 6.0 1.7 5.5 3.25 '2:0']\n",
    " ['BRUSSELS PARIS' 2.25 3.0 3.75 2.8 3.8 2.15 '1:1']\n",
    " ['LISBON VIENNA' 1.45 8.0 4.25 1.9 7.75 2.4 '1:0']\n",
    " ['ROME STOCKHOLM' 1.55 4.75 5.25 1.75 4.75 3.4 '2:2']\n",
    " ['ANKARA PARIS' 2.8 2.35 3.7 3.05 2.7 2.65 '0:4']\n",
    " ['BRUSSELS LONDON' 3.3 2.3 3.2 4.05 2.6 2.25 '0:0']\n",
    " ['BERLIN BERN' 1.35 9.25 4.95 1.65 9.25 2.8 '4:1']\n",
    " ['MADRID ATHENS' 1.4 7.5 5.0 1.7 7.75 2.7 '1:0']\n",
    " ['EDINBURGH OSLO' 4.6 1.75 3.75 4.3 2.15 2.6 '1:2']\n",
    " ['COPENHAGEN AMSTERDAM' 6.0 1.65 3.5 5.5 2.1 2.35 '0:1']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NB_EPOCH = 36\n",
    "INITIAL_EPOCH = 33\n",
    "\n",
    "for step in range(10):\n",
    "    #run Whole Program\n",
    "    # Loss Train\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy'])\n",
    "    history = model.fit(X_train, Y_train,batch_size=BATCH_SIZE, epochs=NB_EPOCH,verbose=VERBOSE,\n",
    "                        validation_split=VALIDATION_SPLIT,initial_epoch = INITIAL_EPOCH)\n",
    "    # Test\n",
    "\n",
    "    score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "    print(\"Test score:\", score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    #model.save('grt1_model.h5')\n",
    "    \n",
    "    \n",
    "    sleep(30)\n",
    "    INITIAL_EPOCH = NB_EPOCH \n",
    "    print(INITIAL_EPOCH)\n",
    "    \n",
    "    NB_EPOCH = INITIAL_EPOCH+3\n",
    "    print(NB_EPOCH)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorFlow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DTA_DIR = '/tmp/data'\n",
    "\n",
    "NUM_STEPS = 2\n",
    "MINIBATCH_SIZE = 100\n",
    "\n",
    "#data = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 21])\n",
    "W = tf.Variable(tf.zeros([21, 3]))\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, [None, 3])\n",
    "y_pred = tf.matmul(x, W)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "logits=y_pred, labels=y_true))\n",
    "\n",
    "\n",
    "gd_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "correct_mask = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "# Train\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for _ in range(NUM_STEPS):\n",
    "        batch_xs, batch_ys = data.train.next_batch(MINIBATCH_SIZE)\n",
    "        sess.run(gd_step, feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "    # Test\n",
    "    ans = sess.run(accuracy, feed_dict={x: data.test.images,y_true: data.test.labels})\n",
    "    \n",
    "print (\"Accuracy: {:.4}%\".format(ans*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
